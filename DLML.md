### Topics

- **Deep Learning for News Clustering and Retrieval:** This project involves building a deep learning pipeline to cluster and retrieve 1,440 science news articles. It utilizes SentenceTransformer embeddings, K-means, and LDA for these tasks.
- **News Clustering:** This is a core task in the pipeline, aiming to group news articles into meaningful categories based on their content. It employs K-means clustering with SentenceTransformer embeddings to achieve this.
- **News Retrieval:** This refers to the system's ability to semantically find and present relevant news articles from an archive. A Retrieval-Augmented Generation (RAG) chatbot is deployed using FAISS and Gemini Pro for real-time question-answering.
- **SentenceTransformer Embeddings:** These are dense numerical vector representations of news articles that capture their semantic meaning. They are generated using the `all-MiniLM-L12-v2` model and are fundamental for clustering and retrieval.
- **Topic Coherence:** This metric assesses the interpretability and quality of topics generated by models like LDA. The project specifically optimized topic coherence, achieving a C_v score of 0.44.
- **Temporal Keyword Trends:** This involves visualizing how keywords associated with identified topics evolve or fluctuate over different time periods. PCA and Streamlit are used to improve the interpretability of these trends and topic distributions.
- **Retrieval-Augmented Generation (RAG):** This is an advanced AI architecture that enhances generative models by incorporating information retrieved from external knowledge sources. A RAG chatbot is deployed using FAISS and Gemini Pro for contextual question-answering on news archives.
- **Semantic Question-Answering:** This capability allows the system to understand the user's query and provide answers that are semantically relevant to the news archive. It is a key function of the RAG chatbot, powered by Gemini Pro.
- **News Scraping:** This is the initial process of automatically extracting news article links and their full content from websites. It involves using `requests` to fetch pages and `BeautifulSoup` to parse HTML, saving data as JSON files.
- **Topic Modeling:** A statistical method used to discover abstract "topics" from a collection of documents. Latent Dirichlet Allocation (LDA) is specifically employed to identify key thematic categories within the news articles.
- **Clustering Quality Metrics Comparison:** This refers to the evaluation and comparison of the performance of clustering and topic modeling algorithms using specific quantitative measures. The project assesses K-means using Silhouette and Davies-Bouldin scores, and LDA using Coherence (C_v).
- **Temporal Trend Analysis:** This analysis examines how the distribution or frequency of specific topics or clusters changes over time. It typically involves plotting the count of articles per topic against their publication dates.

### Methods

- **K-means Clustering:** An unsupervised machine learning algorithm that partitions `n` data points into `k` clusters based on feature similarity. It is used here to group 1,440 science news articles using their SentenceTransformer embeddings.
- **Elbow Method:** A heuristic technique used to determine the optimal number of clusters (`k`) for K-means. It involves plotting the sum of squared distances of samples to their closest cluster center (inertia) against the number of clusters.
- **LDA (Latent Dirichlet Allocation) Topic Modeling:** A generative probabilistic model that discovers the underlying "topics" in a collection of documents. It is applied to enhance topic coherence and assign articles to distinct themes.
- **PCA (Principal Component Analysis):** A dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving most of the variance. It is applied to news article embeddings for 2D visualization of topic clusters.
- **Retrieval-Augmented Generation (RAG):** An architectural pattern for Large Language Models (LLMs) that augments their responses by retrieving relevant information from an external knowledge base. This method is crucial for providing factual and contextual answers in the chatbot.
- **Silhouette Score:** A metric used to evaluate the quality of clusters formed by algorithms like K-means. It measures how similar an object is to its own cluster compared to other clusters, with a score range of -1 to +1, where higher is better.
- **Davies-Bouldin Index:** Another metric used to evaluate the goodness of a clustering result. A lower Davies-Bouldin index value generally indicates better clustering, meaning clusters are compact and well-separated.
- **Coherence Model (C_v):** A specific type of coherence metric used to evaluate the interpretability of topics generated by topic models (e.g., LDA). A higher C_v score suggests that the words within a topic are more semantically related and thus more interpretable.
- **Keyword Extraction:** The process of identifying the most salient and frequent words within specific text segments, such as within each news cluster. This involves filtering out stopwords and counting word occurrences.
- **Web Scraping:** The automated process of extracting structured data from websites. This project uses `requests` to fetch HTML and `BeautifulSoup` to parse it, extracting article titles, links, publication dates, and full text content.

### Modules/Libraries

- **Python:** The core programming language used to develop the entire deep learning pipeline, including data scraping, processing, modeling, visualization, and deployment. It provides the foundational environment for all other libraries.
- **NLP (Natural Language Processing):** This is a broad field of artificial intelligence that deals with the interaction between computers and human language. It encompasses various techniques used in the project, such as text embedding, tokenization, and topic modeling.
- **Streamlit:** An open-source Python framework that enables data scientists and machine learning engineers to create interactive web applications with minimal code. It is used for the user interface, visualizing results, and deploying the chatbot.
- **SentenceTransformers:** A Python library for easy use of state-of-the-art sentence, paragraph, and image embeddings. It is specifically employed to generate high-quality vector representations of news article texts for clustering and retrieval.
- **Gemini Pro (Google Gemini):** A large language model developed by Google AI, designed for various generative AI tasks. In this project, it serves as the powerful engine behind the Retrieval-Augmented Generation (RAG) chatbot.
- **FAISS:** (Facebook AI Similarity Search) A library that provides efficient algorithms for similarity search and clustering of dense vectors. It is integrated with the Gemini Pro model to enable fast and scalable information retrieval for the RAG chatbot.
- **`json`:** A built-in Python module for encoding and decoding JSON (JavaScript Object Notation) data. It is used extensively for loading scraped articles and saving processed data in JSON or JSONL format.
- **`os`:** A built-in Python module that provides a way of using operating system dependent functionality, such as interacting with the file system. It is used for creating directories to store data and plots.
- **`nltk` (Natural Language Toolkit):** A leading platform for building Python programs to work with human language data. It is specifically used for managing and downloading `stopwords` for text preprocessing.
- **`numpy` (as `np`):** The fundamental package for scientific computing with Python, providing support for large, multi-dimensional arrays and matrices. It is used for numerical operations on embeddings and other data structures.
- **`matplotlib.pyplot` (as `plt`):** A comprehensive plotting library for creating static, animated, and interactive visualizations in Python. It is used for generating various plots, including the elbow method, cluster visualizations, and metric comparisons.
- **`collections.Counter`:** A `dict` subclass provided by Python's `collections` module for counting hashable objects. It is used in the keyword extraction process to efficiently count word frequencies within text clusters.
- **`sklearn.cluster.KMeans`:** A class from the `scikit-learn` library that implements the K-means clustering algorithm. This module is directly used to perform the clustering of the news article embeddings.
- **`sklearn.decomposition.PCA`:** A class from `scikit-learn` that implements Principal Component Analysis for dimensionality reduction. It is used to reduce the high-dimensional embeddings to 2D for visualization purposes.
- **`sklearn.metrics.silhouette_score`:** A function from `scikit-learn` that computes the mean Silhouette Coefficient of all samples. It is used to evaluate the quality and density of the formed clusters.
- **`sklearn.metrics.davies_bouldin_score`:** A function from `scikit-learn` that computes the Davies-Bouldin index. This metric provides another way to assess clustering quality, where lower scores indicate better-defined clusters.
- **`pandas` (as `pd`):** A powerful open-source data analysis and manipulation tool, built on top of the Python programming language. It is used for working with DataFrames to load, process, and save structured data like cluster assignments and LDA results.
- **`nltk.corpus.stopwords`:** A sub-module within `nltk` that provides a list of common "stop words" (e.g., "the", "a", "is") in various languages. These words are typically removed from text during preprocessing to focus on more meaningful terms.
- **`sklearn.feature_extraction.text.CountVectorizer`:** A scikit-learn utility that converts a collection of text documents to a matrix of token counts. It is used as a crucial preprocessing step before applying LDA topic modeling.
- **`sklearn.decomposition.LatentDirichletAllocation`:** The scikit-learn implementation of the Latent Dirichlet Allocation (LDA) algorithm. This module is specifically used for performing topic modeling on the news article texts.
- **`gensim.models.CoherenceModel`:** A module from the `gensim` library specifically designed for evaluating the quality and interpretability of topic models. It is used to calculate the C_v coherence score for the LDA topics.
- **`gensim.corpora.Dictionary`:** A utility from the `gensim` library used to create a vocabulary (mapping words to unique IDs) from a corpus of texts. This dictionary is a prerequisite for computing topic coherence.
- **`wordcloud.WordCloud`:** A Python library for creating visual representations of text data where the size of each word indicates its frequency or importance. It is used to generate word clouds for each identified LDA topic.
- **`requests`:** A popular Python library for making HTTP requests to web servers. It is used to fetch web pages during the news scraping process and to interact with external APIs like the Gemini API.
- **`bs4.BeautifulSoup`:** A Python library for parsing HTML and XML documents. It is typically used in conjunction with `requests` to navigate and extract data from web pages during the scraping process.
- **`time`:** A standard Python library providing various time-related functions. It is used to introduce delays during web scraping to avoid overwhelming target servers and to measure response times in the chatbot.
- **`subprocess`:** A Python module that allows you to spawn new processes, connect to their input/output/error pipes, and obtain their return codes. It is used within the Streamlit application to execute other Python scripts (e.g., for scraping or clustering).
- **`serpapi.GoogleSearch`:** A client library for the SerpApi, which provides real-time access to Google search results. It is used by the chatbot to perform web searches and gather relevant context for generating answers.
- **`dotenv.load_dotenv`:** A function from the `python-dotenv` library used to load environment variables from a `.env` file. This helps in securely managing sensitive information like API keys without hardcoding them in the script.